{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing sensitivity of machine learning algorithms and performing a receiver-operator characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With machine learning we may want to change the sensitivity of an algorithm such that a high or lower threshold would be necessary to classify some data as 'positive'. For example, in cancer screening it may be judged to preferable to err towards classifying someone as 'possible cancer' rather than 'probable not cancer' to ensure as few people with cancer are missed by the screen. As we increase the sensitivity in this way we will miss fewer real positives, but at the cost of generating more false positives (those without cancer who are classified as 'possible cancer'). \n",
    "\n",
    "Rather than taking a predicted classification output we take a probability estimate output that is balanced between two classes.  For example the output may be 0.61 for class 1 (no cancer) and 0.39 for class 2 (cancer). Ordinarily the model will classify this patient as ‘no cancer’. But we may set our threshold for classification of cancer at 0.3 for the cancer class, in which case this same patient will be classified as ‘cancer’. That means any classification that has at least a 30% probability of being cancer is classified as cancer.\n",
    "\n",
    "In order to change the sensitivity of the model we train the model as usual, but instead of using .predict to obtain classification, we use .predict_proba. For each patient this will return the probabilities of being either ‘no cancer’ or ‘cancer’. We compare the outputted probability of being cancer with our desired threshold.\n",
    "\n",
    "A ‘receiver operating characteristic (ROC) curve’ is a common way of displaying the trade-off between identifying all true positives, and incorrectly labelling a person as a positive (that they have cancer, for example) when they do not.\n",
    "\n",
    "ROC curve plots two measures:\n",
    "\n",
    "* False positive rate (on the x axis): This is the proportion of real negatives (no cancer) that are mistakenly classified as positive (cancer).\n",
    "\n",
    "* True positive rate (y axis): This is the proportion of true positives (cancer) correctly identified as positive. \n",
    "\n",
    "Ideally we want to identify all true positives with no false positives.\n",
    "\n",
    "In the code below we will train the model, and we will will adjust the threshold of classification in a loop. In order to get the best quality results we will use k-fold stratification to run the model 10 times, with each sample being used once and only as a test sample (rather than training sample). The ‘stratified’ form of the k-fold method ensures that all training/test splits have a consistent split between cancer and non-cancer patients (being representative of the data set as a whole). Please see lesson 75 for more details on the k-fold stratification method.\n",
    "\n",
    "We will use a linear regression model. Note that some other machine learning models, will output their probability classification with .decision_function rather than .predict_proba. Here are the methods for probability output for the common ml methods in sckit learn.\n",
    "\n",
    "* Logistic regression:  predict_proba\n",
    "* Random Forests:  predict_proba\n",
    "* Support Vector Machines:  decision_function\n",
    "* Neural Networks (mlp):  predict_proba\n",
    "\n",
    "Let's look at some code. In order to demonstrate the trade off we'll use the Winsonsin Breast Cancer data set, but we will make it much harder for the model to be accurate by only using the first two features from each patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn has some automated methods for ROC production, but here we will do it 'manually' to see what is happening.\n",
    "\n",
    "For more on ROC curves see https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading modules that will used throughout the code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the function to load the data. Remember we are deliberately only using two features per patient here the make the trade-off in sensitivity of the algoirthm clearer. You would not normally do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load data from appropriate source. Here we load data from scikit\n",
    "    learn's buil in data sets, but we restrict the feature data to two columns\n",
    "    in order to demonstrate changing sensitivity of the model\"\"\"\n",
    "    \n",
    "    from sklearn import datasets\n",
    "    \n",
    "    data_set = datasets.load_breast_cancer()\n",
    "    X=data_set.data[:,0:2]\n",
    "    y=data_set.target\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the function to normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise (X_train,X_test):\n",
    "    \"\"\"Normalise X data, so that training set has mean of zero and standard\n",
    "    deviation of one\"\"\"\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc=StandardScaler() \n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "    # Apply the scaler to the training and test sets\n",
    "    X_train_std=sc.transform(X_train)\n",
    "    X_test_std=sc.transform(X_test)\n",
    "    \n",
    "    # Return normalised data\n",
    "    return X_train_std, X_test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next wi will define the function that will provide us with 'k' (which we will later set to 10) different training/test splits so that each sample is used once and only once as a test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_k_fold_splits(k):\n",
    "    \"\"\"Set up K-fold splits. This will divide data into k sets such that each\n",
    "    data points is used once and only once in the test set. Each test/training\n",
    "    split has same balance of y (the classified outcome) as the whole data\n",
    "    set\"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = k)\n",
    "    skf.get_n_splits(X, y)\n",
    "\n",
    "    return skf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function that will train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model (X, y):\n",
    "    \"\"\"Train the model \"\"\"\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(C=100,random_state=0)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the function that classifies test samples with a given threshold level (which can be anywhere between zero and one). We will put that threshold level in a loop and call this function with different threshold levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model (model, X_test_std, y, threshold):\n",
    "    y_pred_probability = model.predict_proba(X_test_std)\n",
    "    # Check probability of positive classification is >trhreshold\n",
    "    y_pred = (y_pred_probability[:,1] >= threshold)\n",
    "    # Convert boolean to 0/1 (could also simply multiple by 1)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    \n",
    "    test_results = np.vstack((y, y_pred)).T\n",
    "    \n",
    "    # return results\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we will be using just false positive and true positive rates, for reference we'll report back a full list of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diagnostic_performance (actual_predicted):\n",
    "    \"\"\" Calculate diagnostic performance.\n",
    "    \n",
    "    Takes a Numpy array of 1 and zero, two columns: actual and predicted\n",
    "    \n",
    "    Note that some statistics are repeats with different names\n",
    "    (precision = positive_predictive_value and recall = sensitivity).\n",
    "    Both names are returned\n",
    "    \n",
    "    Returns a dictionary of results:\n",
    "        \n",
    "    1) accuracy: proportion of test results that are correct    \n",
    "    2) sensitivity: proportion of true +ve identified\n",
    "    3) specificity: proportion of true -ve identified\n",
    "    4) positive likelihood: increased probability of true +ve if test +ve\n",
    "    5) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    6) true positive rate: proportion of test +ve that are positive\n",
    "    6) false positive rate: proportion of false +ves in true -ve patients\n",
    "    7) false negative rate:  proportion of false -ves in true +ve patients\n",
    "    8) positive predictive value: chance of true +ve if test +ve\n",
    "    9) negative predictive value: chance of true -ve if test -ve\n",
    "    10) precision = positive predictive value \n",
    "    11) recall = sensitivity\n",
    "    12) f1 = (2 * precision * recall) / (precision + recall)\n",
    "    13) positive rate = rate of true +ve (not strictly a performance measure)\n",
    "    \"\"\"\n",
    "    # Calculate results\n",
    "    actual_positives = actual_predicted[:, 0] == 1\n",
    "    actual_negatives = actual_predicted[:, 0] == 0\n",
    "    test_positives = actual_predicted[:, 1] == 1\n",
    "    test_negatives = actual_predicted[:, 1] == 0\n",
    "    test_correct = actual_predicted[:, 0] == actual_predicted[:, 1]\n",
    "    accuracy = np.average(test_correct)\n",
    "    true_positives = actual_positives & test_positives\n",
    "    false_positives = test_positives & actual_negatives\n",
    "    false_negatives = test_negatives & actual_positives\n",
    "    true_negatives = actual_negatives & test_negatives\n",
    "    sensitivity = np.sum(true_positives) / np.sum(actual_positives)\n",
    "    specificity = np.sum(true_negatives) / np.sum(actual_negatives)\n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    true_positive_rate = sensitivity\n",
    "    false_positive_rate = 1 - specificity\n",
    "    false_negative_rate = np.sum(false_negatives) / np.sum(test_negatives)\n",
    "    positive_predictive_value = np.sum(true_positives) / np.sum(test_positives)\n",
    "    negative_predictive_value = np.sum(true_negatives) / np.sum(test_negatives)\n",
    "    precision = positive_predictive_value\n",
    "    recall = sensitivity\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    positive_rate = np.mean(actual_predicted[:,1])\n",
    "    \n",
    "    # Add results to dictionary\n",
    "    performance = {}\n",
    "    performance['accuracy'] = accuracy\n",
    "    performance['sensitivity'] = sensitivity\n",
    "    performance['specificity'] = specificity\n",
    "    performance['positive_likelihood'] = positive_likelihood\n",
    "    performance['negative_likelihood'] = negative_likelihood\n",
    "    performance['true_positive_rate'] = true_positive_rate\n",
    "    performance['false_positive_rate'] = false_positive_rate\n",
    "    performance['false_negative_rate'] = false_negative_rate\n",
    "    performance['positive_predictive_value'] = positive_predictive_value\n",
    "    performance['negative_predictive_value'] = negative_predictive_value\n",
    "    performance['precision'] = precision\n",
    "    performance['recall'] = recall\n",
    "    performance['f1'] = f1\n",
    "    performance['positive_rate'] = positive_rate\n",
    "\n",
    "    # Return results dictionary\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calulate the area under the curve of the ROC curve. For a perfect classification this would equal 1.0. If the model is no better, but no worse, than chance then area under curve would be zero. For a model that is worse than chance the area under curve would be less than 0.5. We use the NumPy trapz method to calculate area under curve which is a suitable method when we are determining the ROC curve in high detail as we are here (we will have 100 points on the ROC curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area_under_roc (true_positive_rate, false_positive_rate):\n",
    "    auc = np.trapz(true_positive_rate, x=false_positive_rate)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will record all results in a Pandas dataframe. Here we set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_results_dataframe():\n",
    "    cols = ['threshold',\n",
    "            'k_fold',\n",
    "            'accuracy',\n",
    "            'sensitivity',\n",
    "            'specificity',\n",
    "            'positive_likelihood',\n",
    "            'negative_likelihood',\n",
    "            'true_positive_rate',\n",
    "            'false_positive_rate',\n",
    "            'false_negative_rate',\n",
    "            'positive_predictive_value',\n",
    "            'negative_predictive_value',\n",
    "            'precision',\n",
    "            'recall',\n",
    "            'f1',\n",
    "            'positive_rate']\n",
    "    \n",
    "    df = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a function to plot the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(false_positive_rate, true_positive_rate):\n",
    "    \"\"\"Plot receiver operator curve.\"\"\"\n",
    "    \n",
    "    %matplotlib inline\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.ticker as ticker\n",
    "    \n",
    "\n",
    "    x = false_positive_rate\n",
    "    y = true_positive_rate\n",
    "    \n",
    "    # Define the size of the overall figure\n",
    "    fig = plt.figure(figsize=(5,5)) \n",
    "\n",
    "    # Create subplot 1\n",
    "\n",
    "    ax1 = fig.add_subplot(111) # Grid of 2x2, this is suplot 1\n",
    "    \n",
    "    # Plot mran results\n",
    "    ax1.plot(x, y, color = 'k')\n",
    "    \n",
    "    ax1.set_xlabel('False positive rate')\n",
    "    ax1.set_ylabel('True positive rate rate')\n",
    "    ax1.xaxis.set_major_locator(ticker.MultipleLocator(0.2))\n",
    "    ax1.xaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "    ax1.yaxis.set_major_locator(ticker.MultipleLocator(0.2))\n",
    "    ax1.yaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "    ax1.grid(True, which='both')\n",
    "    \n",
    "    plt.savefig('roc.png',dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will call those functions in the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/michael/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/michael/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/michael/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/home/michael/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/michael/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73de9c7710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under curve mean = 0.956\n",
      "Area under curve stdev = 0.020\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_data()\n",
    "\n",
    "# Set up K-fold splits. \n",
    "splits = 10\n",
    "skf = set_up_k_fold_splits (splits)\n",
    "\n",
    "# Set up results dataframe\n",
    "\n",
    "results_df = set_up_results_dataframe()\n",
    "\n",
    "# Set up counters (used in the results record)\n",
    "\n",
    "run_id = 0\n",
    "k_fold_count = 0\n",
    "\n",
    "# Loop through k-fold training/test splits\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    # Divide data in train and test\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Normalise data\n",
    "    X_train_std, X_test_std = normalise(X_train,X_test)\n",
    "\n",
    "    # Train model\n",
    "    model = train_model (X_train_std,y_train)\n",
    "    \n",
    "    # Loop through a range of thresholds when predicting classification\n",
    "    for threshold in np.arange (0.0,1.01,0.01):\n",
    "        # linear regression model has .predict+proba  method to return \n",
    "        # probability of outcomes. Some methods such as svc use \n",
    "        # .decision_function to return probability\n",
    "        \n",
    "        # Get test results (combined with actual classification)\n",
    "        test_results = test_model (model, X_test_std, y_test, threshold)\n",
    "        \n",
    "        # Measure performance of test set predictions\n",
    "        performance = calculate_diagnostic_performance(test_results)\n",
    "        run_results = [threshold, k_fold_count] + list(performance.values())\n",
    "        results_df.loc[run_id] = run_results\n",
    "        run_id += 1\n",
    "    \n",
    "    k_fold_count += 1 # increment counter of k-fold\n",
    "\n",
    "# Summarise performance data\n",
    "summary_mean = results_df.groupby('threshold').mean()\n",
    "summary_stdev = results_df.groupby('threshold').std()\n",
    "\n",
    "# Call for ROC to be plotted \n",
    "plot_roc (summary_mean['false_positive_rate'],\n",
    "          summary_mean['true_positive_rate'])\n",
    "    \n",
    "# Calculate area under curve for each k-fold\n",
    "area_under_curve_per_run = []\n",
    "for split in range(splits):\n",
    "    mask = results_df['k_fold'] == split\n",
    "    run_data = results_df[mask]\n",
    "    run_data = run_data.sort_values(['false_positive_rate'])\n",
    "    true_positive_rate = run_data['true_positive_rate'].values\n",
    "    false_positive_rate = run_data['false_positive_rate'].values\n",
    "    auc = calculate_area_under_roc(true_positive_rate, false_positive_rate)\n",
    "    area_under_curve_per_run.append(auc)\n",
    "    area_under_curve_mean = np.mean(area_under_curve_per_run)\n",
    "    area_under_curve_stdev = np.std(area_under_curve_per_run)\n",
    "print ('Area under curve mean = %0.3f' %area_under_curve_mean)\n",
    "print ('Area under curve stdev = %0.3f' %area_under_curve_stdev)\n",
    "    \n",
    "# Save data \n",
    "results_df.to_csv('all_roc_results.csv')\n",
    "summary_mean.to_csv('roc_summary_mean.csv')\n",
    "summary_stdev.to_csv('roc_summary_stdev.csv')\n",
    "auc_df = pd.DataFrame(area_under_curve_per_run)\n",
    "auc_df.to_csv('roc_auc_per_kfold_split.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
